{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SparkMagic\n",
    "\n",
    "More info here: https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>send_to_spark</td>\n",
       "    <td>%%send_to_spark -o variable -t str -n var</td>\n",
       "    <td>Sends a variable from local output to spark cluster.\n",
       "    <br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-i VAR_NAME: Local Pandas DataFrame(or String) of name VAR_NAME will be available in the %%spark context as a \n",
       "          Spark dataframe(or String) with the same name.</li>\n",
       "        <li>-t TYPE: Specifies the type of variable passed as -i. Available options are:\n",
       "         `str` for string and `df` for Pandas DataFrame. Optional, defaults to `str`.</li>\n",
       "        <li>-n NAME: Custom name of variable passed as -i. Optional, defaults to -i variable name.</li>\n",
       "        <li>-m MAXROWS: Maximum amount of Pandas rows that will be sent to Spark. Defaults to 2500.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'remotesparkmagics-sample', 'executorMemory': '4G', 'executorCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"executorCores\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135449d3143544c5afcc82a1b0c9897c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c2feaaa882469b9948409a7e9614fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql -o query1\n",
    "SELECT 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'remotesparkmagics-sample', 'executorMemory': '4G', 'executorCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td></td></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.1442"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 100000\n",
    "def sample(p):\n",
    "  x, y = random.random(), random.random()\n",
    "  return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "print(f\"Pi is roughly {4.0 * count / NUM_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql -o tables -q\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "print(len(query1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "variable = 'this string will be available in spark cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'variable' as 'custom_name' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i variable -n custom_name -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this string will be available in spark cluster"
     ]
    }
   ],
   "source": [
    "print(custom_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/04/29 19:15:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/04/29 19:15:33 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/04/29 19:15:33 INFO SparkContext: Submitted application: remotesparkmagics-sample\n",
      "20/04/29 19:15:33 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/04/29 19:15:33 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/04/29 19:15:33 INFO SecurityManager: Changing view acls groups to: \n",
      "20/04/29 19:15:33 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/04/29 19:15:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/04/29 19:15:34 INFO Utils: Successfully started service 'sparkDriver' on port 42215.\n",
      "20/04/29 19:15:34 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/04/29 19:15:34 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/04/29 19:15:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/04/29 19:15:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/04/29 19:15:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3b5a7a3f-cf0a-4683-ad90-72e4959a125d\n",
      "20/04/29 19:15:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/04/29 19:15:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/04/29 19:15:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/04/29 19:15:34 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "20/04/29 19:15:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://20aab232ab07:4041\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-api-0.7.0-incubating.jar at spark://20aab232ab07:42215/jars/livy-api-0.7.0-incubating.jar with timestamp 1588187734637\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-rsc-0.7.0-incubating.jar at spark://20aab232ab07:42215/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1588187734642\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar at spark://20aab232ab07:42215/jars/livy-thriftserver-session-0.7.0-incubating.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/netty-all-4.0.37.Final.jar at spark://20aab232ab07:42215/jars/netty-all-4.0.37.Final.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar at spark://20aab232ab07:42215/jars/commons-codec-1.9.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/livy-core_2.11-0.7.0-incubating.jar at spark://20aab232ab07:42215/jars/livy-core_2.11-0.7.0-incubating.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:34 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/livy-repl_2.11-0.7.0-incubating.jar at spark://20aab232ab07:42215/jars/livy-repl_2.11-0.7.0-incubating.jar with timestamp 1588187734644\n",
      "20/04/29 19:15:34 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/04/29 19:15:34 INFO Executor: Using REPL class URI: spark://20aab232ab07:42215/classes\n",
      "20/04/29 19:15:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40125.\n",
      "20/04/29 19:15:34 INFO NettyBlockTransferService: Server created on 20aab232ab07:40125\n",
      "20/04/29 19:15:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/04/29 19:15:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 20aab232ab07, 40125, None)\n",
      "20/04/29 19:15:34 INFO BlockManagerMasterEndpoint: Registering block manager 20aab232ab07:40125 with 366.3 MB RAM, BlockManagerId(driver, 20aab232ab07, 40125, None)\n",
      "20/04/29 19:15:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 20aab232ab07, 40125, None)\n",
      "20/04/29 19:15:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 20aab232ab07, 40125, None)\n",
      "20/04/29 19:15:35 INFO SparkEntries: Spark context finished initialization in 1369ms\n",
      "20/04/29 19:15:35 INFO SparkEntries: Created Spark session.\n",
      "20/04/29 19:15:45 INFO SparkEntries: Created SQLContext.\n",
      "20/04/29 19:15:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/spark-warehouse').\n",
      "20/04/29 19:15:45 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.\n",
      "20/04/29 19:15:46 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/04/29 19:15:49 INFO CodeGenerator: Code generated in 493.395338 ms\n",
      "20/04/29 19:15:49 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:153)\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Missing parents: List()\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/04/29 19:15:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.9 KB, free 366.3 MB)\n",
      "20/04/29 19:15:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.5 KB, free 366.3 MB)\n",
      "20/04/29 19:15:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 20aab232ab07:40125 (size: 8.5 KB, free: 366.3 MB)\n",
      "20/04/29 19:15:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163\n",
      "20/04/29 19:15:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/04/29 19:15:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/04/29 19:15:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8080 bytes)\n",
      "20/04/29 19:15:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/livy-thriftserver-session-0.7.0-incubating.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:49 INFO TransportClientFactory: Successfully created connection to 20aab232ab07/172.17.0.2:42215 after 20 ms (0 ms spent in bootstraps)\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/livy-thriftserver-session-0.7.0-incubating.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp2376037152791269738.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/livy-thriftserver-session-0.7.0-incubating.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/commons-codec-1.9.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/commons-codec-1.9.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp2612101829443620938.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/commons-codec-1.9.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1588187734642\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/livy-rsc-0.7.0-incubating.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp6089066586520039856.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/livy-rsc-0.7.0-incubating.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/netty-all-4.0.37.Final.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/netty-all-4.0.37.Final.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp1684306595984315418.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/netty-all-4.0.37.Final.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/livy-api-0.7.0-incubating.jar with timestamp 1588187734637\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/livy-api-0.7.0-incubating.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp4207279015294743848.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/livy-api-0.7.0-incubating.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/livy-repl_2.11-0.7.0-incubating.jar with timestamp 1588187734644\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/livy-repl_2.11-0.7.0-incubating.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp6283234653933357519.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/livy-repl_2.11-0.7.0-incubating.jar to class loader\n",
      "20/04/29 19:15:49 INFO Executor: Fetching spark://20aab232ab07:42215/jars/livy-core_2.11-0.7.0-incubating.jar with timestamp 1588187734643\n",
      "20/04/29 19:15:49 INFO Utils: Fetching spark://20aab232ab07:42215/jars/livy-core_2.11-0.7.0-incubating.jar to /tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/fetchFileTemp7689971353929527807.tmp\n",
      "20/04/29 19:15:49 INFO Executor: Adding file:/tmp/spark-d3235ccb-ef5f-4f9c-aed6-56b11931dd47/userFiles-b7f08ce9-3305-449d-82ac-2d1cf26cb12b/livy-core_2.11-0.7.0-incubating.jar to class loader\n",
      "20/04/29 19:15:49 INFO CodeGenerator: Code generated in 23.634701 ms\n",
      "20/04/29 19:15:50 INFO CodeGenerator: Code generated in 26.900127 ms\n",
      "20/04/29 19:15:50 INFO CodeGenerator: Code generated in 109.794443 ms\n",
      "20/04/29 19:15:50 INFO PythonRunner: Times: total = 654, boot = 457, init = 197, finish = 0\n",
      "20/04/29 19:15:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1789 bytes result sent to driver\n",
      "20/04/29 19:15:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1121 ms on localhost (executor driver) (1/1)\n",
      "20/04/29 19:15:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/04/29 19:15:50 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43789\n",
      "20/04/29 19:15:50 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:153) finished in 1.426 s\n",
      "20/04/29 19:15:50 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 1.488143 s"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%delete -f -s 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
