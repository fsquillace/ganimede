{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SparkMagic\n",
    "\n",
    "More info here: https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>send_to_spark</td>\n",
       "    <td>%%send_to_spark -o variable -t str -n var</td>\n",
       "    <td>Sends a variable from local output to spark cluster.\n",
       "    <br/>\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-i VAR_NAME: Local Pandas DataFrame(or String) of name VAR_NAME will be available in the %%spark context as a \n",
       "          Spark dataframe(or String) with the same name.</li>\n",
       "        <li>-t TYPE: Specifies the type of variable passed as -i. Available options are:\n",
       "         `str` for string and `df` for Pandas DataFrame. Optional, defaults to `str`.</li>\n",
       "        <li>-n NAME: Custom name of variable passed as -i. Optional, defaults to -i variable name.</li>\n",
       "        <li>-m MAXROWS: Maximum amount of Pandas rows that will be sent to Spark. Defaults to 2500.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'remotesparkmagics-sample', 'executorMemory': '4G', 'executorCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"executorCores\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27fa5c99538474e84935df85601769f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac4d10d9b9a489d9172aae43dcf046a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql -o query1\n",
    "SELECT 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'remotesparkmagics-sample', 'executorMemory': '4G', 'executorCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "print(len(query1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "variable = 'this string will be available in spark cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'variable' as 'custom_name' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i variable -n custom_name -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this string will be available in spark cluster"
     ]
    }
   ],
   "source": [
    "print(custom_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/04/24 18:57:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/04/24 18:58:00 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/04/24 18:58:00 INFO SparkContext: Submitted application: remotesparkmagics-sample\n",
      "20/04/24 18:58:00 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/04/24 18:58:00 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/04/24 18:58:00 INFO SecurityManager: Changing view acls groups to: \n",
      "20/04/24 18:58:00 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/04/24 18:58:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/04/24 18:58:00 INFO Utils: Successfully started service 'sparkDriver' on port 34121.\n",
      "20/04/24 18:58:00 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/04/24 18:58:00 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/04/24 18:58:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/04/24 18:58:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/04/24 18:58:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-662df4ae-dbd4-4177-ab80-0205aa996876\n",
      "20/04/24 18:58:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/04/24 18:58:00 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/04/24 18:58:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/04/24 18:58:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://d2b1e510f4b2:4040\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/netty-all-4.0.37.Final.jar at spark://d2b1e510f4b2:34121/jars/netty-all-4.0.37.Final.jar with timestamp 1587754680852\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-api-0.7.0-incubating.jar at spark://d2b1e510f4b2:34121/jars/livy-api-0.7.0-incubating.jar with timestamp 1587754680866\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-rsc-0.7.0-incubating.jar at spark://d2b1e510f4b2:34121/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar at spark://d2b1e510f4b2:34121/jars/livy-thriftserver-session-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/livy-core_2.11-0.7.0-incubating.jar at spark://d2b1e510f4b2:34121/jars/livy-core_2.11-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar at spark://d2b1e510f4b2:34121/jars/commons-codec-1.9.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:00 INFO SparkContext: Added JAR file:///home/jovyan/livy/apache-livy-0.7.0-incubating-bin/repl_2.11-jars/livy-repl_2.11-0.7.0-incubating.jar at spark://d2b1e510f4b2:34121/jars/livy-repl_2.11-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:01 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/04/24 18:58:01 INFO Executor: Using REPL class URI: spark://d2b1e510f4b2:34121/classes\n",
      "20/04/24 18:58:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37471.\n",
      "20/04/24 18:58:01 INFO NettyBlockTransferService: Server created on d2b1e510f4b2:37471\n",
      "20/04/24 18:58:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/04/24 18:58:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d2b1e510f4b2, 37471, None)\n",
      "20/04/24 18:58:01 INFO BlockManagerMasterEndpoint: Registering block manager d2b1e510f4b2:37471 with 366.3 MB RAM, BlockManagerId(driver, d2b1e510f4b2, 37471, None)\n",
      "20/04/24 18:58:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d2b1e510f4b2, 37471, None)\n",
      "20/04/24 18:58:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d2b1e510f4b2, 37471, None)\n",
      "20/04/24 18:58:01 INFO SparkEntries: Spark context finished initialization in 1345ms\n",
      "20/04/24 18:58:01 INFO SparkEntries: Created Spark session.\n",
      "20/04/24 18:58:10 INFO SparkEntries: Created SQLContext.\n",
      "20/04/24 18:58:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/livy/apache-livy-0.7.0-incubating-bin/spark-warehouse/').\n",
      "20/04/24 18:58:10 INFO SharedState: Warehouse path is 'file:/home/jovyan/livy/apache-livy-0.7.0-incubating-bin/spark-warehouse/'.\n",
      "20/04/24 18:58:11 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/04/24 18:58:13 INFO CodeGenerator: Code generated in 301.272965 ms\n",
      "20/04/24 18:58:13 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/04/24 18:58:13 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/04/24 18:58:13 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:153)\n",
      "20/04/24 18:58:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/04/24 18:58:13 INFO DAGScheduler: Missing parents: List()\n",
      "20/04/24 18:58:13 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/04/24 18:58:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.9 KB, free 366.3 MB)\n",
      "20/04/24 18:58:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.5 KB, free 366.3 MB)\n",
      "20/04/24 18:58:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d2b1e510f4b2:37471 (size: 8.5 KB, free: 366.3 MB)\n",
      "20/04/24 18:58:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163\n",
      "20/04/24 18:58:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/04/24 18:58:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/04/24 18:58:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8080 bytes)\n",
      "20/04/24 18:58:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:14 INFO TransportClientFactory: Successfully created connection to d2b1e510f4b2/172.17.0.2:34121 after 14 ms (0 ms spent in bootstraps)\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/livy-rsc-0.7.0-incubating.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp7611845764787744427.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/livy-rsc-0.7.0-incubating.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/netty-all-4.0.37.Final.jar with timestamp 1587754680852\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/netty-all-4.0.37.Final.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp2827250595494744352.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/netty-all-4.0.37.Final.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/livy-api-0.7.0-incubating.jar with timestamp 1587754680866\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/livy-api-0.7.0-incubating.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp4014804573999361905.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/livy-api-0.7.0-incubating.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/livy-repl_2.11-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/livy-repl_2.11-0.7.0-incubating.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp2279299011985155942.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/livy-repl_2.11-0.7.0-incubating.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/livy-core_2.11-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/livy-core_2.11-0.7.0-incubating.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp6988920677644106580.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/livy-core_2.11-0.7.0-incubating.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/commons-codec-1.9.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/commons-codec-1.9.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp6638262798807397453.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/commons-codec-1.9.jar to class loader\n",
      "20/04/24 18:58:14 INFO Executor: Fetching spark://d2b1e510f4b2:34121/jars/livy-thriftserver-session-0.7.0-incubating.jar with timestamp 1587754680867\n",
      "20/04/24 18:58:14 INFO Utils: Fetching spark://d2b1e510f4b2:34121/jars/livy-thriftserver-session-0.7.0-incubating.jar to /tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/fetchFileTemp7613337398561929946.tmp\n",
      "20/04/24 18:58:14 INFO Executor: Adding file:/tmp/spark-163c28a3-bac7-4123-88c6-bf3fb8cdbb49/userFiles-8519ed65-101c-41ad-b9cd-0f67a1b40bfd/livy-thriftserver-session-0.7.0-incubating.jar to class loader\n",
      "20/04/24 18:58:14 INFO CodeGenerator: Code generated in 28.065821 ms\n",
      "20/04/24 18:58:14 INFO CodeGenerator: Code generated in 39.659397 ms\n",
      "20/04/24 18:58:15 INFO CodeGenerator: Code generated in 68.628445 ms\n",
      "20/04/24 18:58:15 INFO PythonRunner: Times: total = 828, boot = 695, init = 132, finish = 1\n",
      "20/04/24 18:58:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1832 bytes result sent to driver\n",
      "20/04/24 18:58:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1501 ms on localhost (executor driver) (1/1)\n",
      "20/04/24 18:58:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/04/24 18:58:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59795\n",
      "20/04/24 18:58:15 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:153) finished in 1.851 s\n",
      "20/04/24 18:58:15 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 1.920100 s"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%delete -f -s 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
